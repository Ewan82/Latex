%Chapter 3


\section{Data assimilation methods}

Data assimilation provides techniques for combining observations and prior knowledge of a system in an optimal way to find an improved estimate of the system. The prior knowledge of a system often takes the form of a numerical model and an initial guess of the model state/parameters. Many statistical methods have been developed for data assimilation. These methods can largely be categorised as either sequential or variational. Sequential algorithms solve the system of equations needed to find an optimal solution explicitly at each observation time. Variational methods solve the equations needed for an optimal solution implicitly by minimising a cost function for all available observations over some time window. This thesis is mainly concerned with the variational technique of four-dimensional variational data assimilation (4D-Var). 

In numerical weather prediction data assimilation has been predominately used for state estimation whilst keeping parameters fixed. This is because numerical weather prediction is mainly dependent on the initial state with model physics being well understood. Ecosystem carbon cycle models are more dependent on finding the correct set of parameters to describe the ecosystem of interest \citep{luo2015predictability}. We therefore discuss data assimilation for joint state and parameter estimation. In the next sections (\ref{chap3:sec:intro_da} to \ref{chap3:sec:mcmc_seq}) we give a general introduction to data assimilation, then expand this to 4D-Var and finally we briefly discuss other data assimilation methods not directly used in this thesis but applicable to subsequent discussion. 

\subsection{Introduction to data assimilation} \label{chap3:sec:intro_da}

We consider a system that can be described by a numerical model with a true model state \(\textbf{z}^{t} \in \mathbb{R}^{n}\) and true parameters \(\textbf{p}^{t} \in \mathbb{R}^{q}\). We then define the true augmented state as
\begin{equation}
\textbf{x}^{t} =
\begin{pmatrix}
\textbf{p}^{t} \\
\textbf{z}^{t}
\end{pmatrix}
\in \mathbb{R}^{q+n}.
\end{equation}
The initial guess to this model augmented state \(\textbf{x}^{b} \in \mathbb{R}^{q+n}\) (often referred to as the prior or background) and observations of the system \(\textbf{y} \in \mathbb{R}^{m}\) will only be approximations to the true system state, such that
\begin{equation}
\textbf{x}^{b} = \textbf{x}^{t} + \bm{\epsilon}^{b}, \label{chap3:eqn:xb}
\end{equation} 
\begin{equation}
\textbf{y} = h(\textbf{x}^{t}) + \bm{\epsilon}^{o}, \label{chap3:eqn:y}
\end{equation} 
where \( \bm{\epsilon}^{b}\) and \( \bm{\epsilon}^{o}\) are the prior and observation errors respectively, and \(h: \mathbb{R}^{q+n}\rightarrow \mathbb{R}^{m}\) is the observation operator (can be linear or non-linear) mapping the augmented state to observation space, for example the nonlinear mapping of carbon pool state and parameters to eddy covariance derived observations of NEE. The errors in the prior and observations are assumed to be unbiased and mutually independent with known covariance matrices \(\textbf{B} = \mathbb{E}[\bm{\epsilon}^{b}(\bm{\epsilon}^{b})^{T}]\) and \(\textbf{R} = \mathbb{E}[\bm{\epsilon}^{o}(\bm{\epsilon}^{o})^{T}]\).

The best estimate to \(\textbf{x}^{t}\) satisfying both equation~\eqref{chap3:eqn:xb} and \eqref{chap3:eqn:y} is often called the analysis or the posterior estimate, here denoted \(\textbf{x}^{a}\). It is possible to derive this analysis by applying Bayesian methods to probability density functions. Bayes' theorem is first discussed in \citet{bayes1763} but formalised by \citet{laplace1781memoire}. Applied to probability density functions (pdf's) Bayes theorem can be expressed mathematically as
\begin{equation}
p^a(\textbf{x}|\textbf{y}) \propto p^b(\textbf{x})p^o(\textbf{y}|\textbf{x}), \label{chap3:eqn:bayes}
\end{equation}
where \(p^b(\textbf{x})\) is the pdf for the prior, \(p^o(\textbf{y}|\textbf{x})\) is the pdf of the observations given the augmented state and \(p^a(\textbf{x}|\textbf{y})\) is the posterior pdf for the augmented state. Maximising the probability \(p^a(\textbf{x}|\textbf{y})\) is then equivalent to finding the augmented state that best represents the observations. 

If we make the assumption of Gaussian probability density functions with
\begin{equation}
p^{b}(\textbf{x}) = \frac{1}{\sqrt{2\pi |\textbf{B}|^{(q+n)}}}\text{exp}\big(-\frac{1}{2}(\textbf{x}-\textbf{x}^{b})^{T}\textbf{B}^{-1}(\textbf{x}-\textbf{x}^{b})\big)
\end{equation}
and
\begin{equation}
p^{o}(\textbf{y}|\textbf{x}) = \frac{1}{\sqrt{2\pi |\textbf{R}|^{m}}}\text{exp}\big(-\frac{1}{2}(\textbf{y}-h(\textbf{x}))^{T}\textbf{R}^{-1}(\textbf{y}-h(\textbf{x}))\big).
\end{equation}
Then from Bayes' theorem (equation~\eqref{chap3:eqn:bayes}) the posterior probability density function for the augmented state
\begin{equation}
p^{a}(\textbf{x}|\textbf{y}) \propto \text{exp}\big(-\frac{1}{2}(\textbf{x}-\textbf{x}^{b})^{T}\textbf{B}^{-1}(\textbf{x}-\textbf{x}^{b})-\frac{1}{2}(\textbf{y}-h(\textbf{x}))^{T}\textbf{R}^{-1}(\textbf{y}-h(\textbf{x}))\big), \label{chap3:eqn:p_x_y}
\end{equation} 
here we can ignore the constant multiplying the exponential function as it is independent of \textbf{x}. We want to maximise the probability of the augmented state \textbf{x} given the observations \textbf{y}. From equation~\eqref{chap3:eqn:p_x_y} we can see that to maximise \(p^{a}(\textbf{x}|\textbf{y})\) we must maximise the terms in the exponent, this is equivalent to minimising the quadratic cost function 
\begin{equation}
J(\textbf{x}) = \frac{1}{2}(\textbf{x}-\textbf{x}^{b})^{T}\textbf{B}^{-1}(\textbf{x}-\textbf{x}^{b}) + \frac{1}{2}(\textbf{y}-h(\textbf{x}))^{T}\textbf{R}^{-1}(\textbf{y}-h(\textbf{x})). \label{chap3:eqn:3dvar}
\end{equation}
This is the cost function minimised in three-dimensional variational data assimilation (3D-Var), where the minimum is found using a descent algorithm evaluating equation~\eqref{chap3:eqn:3dvar} and its gradient \citep{courtier1998ecmwf}. We can approximate the minimum of \eqref{chap3:eqn:3dvar} by finding its gradient and setting it to zero to obtain the Best Linear Unbiased Estimate (BLUE) \citep{talagrand1997assimilation} where
\begin{equation}
\textbf{x}^{a} = \textbf{x}^{b} + \textbf{K}(\textbf{y} - h(\textbf{x}^{b})), \label{chap3:eqn:blue}
\end{equation}
\begin{equation}
\textbf{K} = \textbf{B}\textbf{H}^{T}(\textbf{H}\textbf{B}\textbf{H}^{T}+\textbf{R})^{-1},
\end{equation}
where \textbf{K} is the Kalman gain matrix specifying the weight of the analysis increment and \(\textbf{H}=\frac{\partial h(\textbf{x})}{\partial \textbf{x}}\) is the linearised observation operator (linearised around \(\textbf{x}=\textbf{x}^{b}\)). We can also approximate the analysis error covariance matrix as
\begin{equation}
\textbf{A} = (\textbf{H}^{T}\textbf{R}^{-1}\textbf{H}+\textbf{B}^{-1})^{-1}. \label{chap3:eqn:a_cov}
\end{equation}
If \( h \) is linear then \eqref{chap3:eqn:blue} and \eqref{chap3:eqn:a_cov} are exact solutions.
%(BLUE) introduce basic concept of DA for a linear Gaussian time-invariant system... will use this in Info Con chapter

%BLUE \(\rightarrow\) 3D-Var

\subsection{4D-Var} \label{chap3:sec:4dvar}

%*Brief* but inclusive of all notation need for results chapter on information content
Four dimensional variational data assimilation (4D-Var) extends 3D-Var to allow for the assimilation of observations distributed throughout some time interval \(t_{0}\) to \(t_{N}\). \citet{Sasaki70somebasic} proposed a method for combining a time series of observations with a numerical model, which was then further developed for use in numerical weather prediction \citep{dimet1986variational}. In 4D-Var we minimise the cost function,
\begin{equation}
\begin{split}
J(\textbf{x}_0) &= J_b(\textbf{x}_0) + J_o(\textbf{x}_0) \\
&= \frac{1}{2}(\textbf{x}_0-\textbf{x}^b)^{T}\textbf{B}^{-1}(\textbf{x}_0-\textbf{x}^b)+\frac{1}{2}\sum_{i=0}^{N}(\textbf{y}_i-\textbf{h}_i(\textbf{x}_i))^{T}\textbf{R}_{i,i}^{-1}(\textbf{y}_i-\textbf{h}_i(\textbf{x}_i)), \label{chap3:eqn:4dvar_cost}
\end{split}
\end{equation}
to obtain the analysis \(\textbf{x}^{a}_{0}\), valid at the initial time \(t_{0}\), subject to the strong constraint that the model states (\(\textbf{x}_0, \dots, \textbf{x}_N\)) must satisfy the model equations,
\begin{equation}
\textbf{x}_{i} = \textbf{m}_{i-1 \rightarrow i}(\textbf{x}_{i-1}), \label{chap3:eqn:nonlinmod}
\end{equation}
where \(\textbf{x}_{i}\) is the model augmented state at time \(t_i\), \(\textbf{m}_{i-1 \rightarrow i}\) is the possibly nonlinear augmented system model evolving \(\textbf{x}_{i-1}\) from time \(t_{i-1}\) to time \(t_i\), \(\textbf{y}_i\) is the vector of observations at time \(t_i\), \(h_i\) is the observation operator at time \(t_i\), and \(\textbf{R}_{i,i}\) is the observation error covariance matrix at time \(t_i\). The time evolution model for the parameter components of the augmented system is just \(\textbf{p}_i = \textbf{p}_{i-1}\), so that the model parameters are constant in time. We can generalise equation~\eqref{chap3:eqn:4dvar_cost} to avoid the sum notation as
\begin{equation}
J(\textbf{x}_0) = \frac{1}{2}(\textbf{x}_0-\textbf{x}^b)^{T}\textbf{B}^{-1}(\textbf{x}_0-\textbf{x}^b)+\frac{1}{2}(\hat{\textbf{y}}-\hat{\textbf{h}}(\textbf{x}_0))^{T}\hat{\textbf{R}}^{-1}(\hat{\textbf{y}}-\hat{\textbf{h}}(\textbf{x}_0)), \label{chap3:costfn}
\end{equation}
where,
\begin{equation}
\hat{\textbf{y}} =
\begin{pmatrix}
\textbf{y}_0 \\
\textbf{y}_1\\
\vdots \\
\textbf{y}_N
\end{pmatrix},
\hspace{1mm}
\hat{\textbf{h}}(\textbf{x}_0)=
\begin{pmatrix}
\textbf{h}_0(\textbf{x}_0) \\
\textbf{h}_1(\textbf{m}_{0\rightarrow 1}(\mathbf{x}_{0}))\\
\vdots \\
\textbf{h}_N(\textbf{m}_{0\rightarrow N}(\mathbf{x}_{0}))
\end{pmatrix}, \\
\hspace{1mm} \text{and} \hspace{3mm}
\hat{\mathbf{R}} =
\begin{pmatrix}
\mathbf{R}_{0, 0} & \mathbf{R}_{0, 1} & \dots & \mathbf{R}_{0, N} \\
\mathbf{R}_{1, 0} & \mathbf{R}_{1, 1} & \dots & \mathbf{R}_{1, N} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{R}_{N, 0} & \mathbf{R}_{N, 1} & \dots & \mathbf{R}_{N, N}
\end{pmatrix},
\end{equation}
with the off-diagonal blocks of \(\hat{\mathbf{R}}\) corresponding to correlations in time between observation errors. For 4D-Var we approximate the analysis error covariance matrix as
\begin{equation}
\textbf{A} = (\hat{\textbf{H}}^{T}\hat{\textbf{R}}^{-1}\hat{\textbf{H}}+\textbf{B}^{-1})^{-1}, \label{chap3:eqn:a_cov_4dvar}
\end{equation}
where \(\hat{\textbf{H}}\) is the observability matrix given by
\begin{equation}
\hat{\mathbf{H}}=
\begin{pmatrix}
\mathbf{H}_0 \\
\mathbf{H}_1\mathbf{M}_0\\
\vdots \\
\mathbf{H}_N\mathbf{M}_{N,0}
\end{pmatrix}
\end{equation}
with $\textbf{H}_i = \frac{\partial \textbf{h}_i(\textbf{x}_i)}{\partial\textbf{x}_i}$ the linearised observation operator and $\mathbf{M}_{i,0}=\mathbf{M}_{i-1}\mathbf{M}_{i-2}\cdots\mathbf{M}_0$ the tangent linear model with $\mathbf{M}_i=\frac{\partial \textbf{m}_{i-1\rightarrow i}(\textbf{x}_{i})}{\partial \textbf{x}_{i}}$. The tangent linear model can be difficult to implement, however using techniques such as automatic differentiation \citep{renaud1997automatic} can reduce the time taken to implement the derivative of a model. These techniques are employed in this thesis.

\subsection{Markov chain Monte Carlo and sequential approaches} \label{chap3:sec:mcmc_seq}

Markov chain Monte Carlo (MCMC) methods refer to a suite of related algorithms (Metropolis-Hastings, simulated annealing and Gibbs sampling), with one of the first MCMC methods being the Metropolis algorithm \citep{metropolis1953equation}. These methods sample the posterior pdf by calculating a cost function measuring the model-data mismatch at different points, usually similar to \(-J_o(\textbf{x}_0)\) shown in equation~\eqref{chap3:eqn:4dvar_cost}. As these methods use \(-J_o(\textbf{x}_0)\) they seek to find a global optimum for this cost function, rather than a minimum. This is achieved by iteratively sampling the cost function, with each iteration of the parameter and state values being uniquely determined by the previously sampled parameter and state values. The output of the MCMC methods is a set of accepted parameter and state values from which analysis or posterior error covariances can be calculated. These methods are easy to implement and do not require the derivative of the model code. However, they come with high computational cost as they often require in the order of \(10^{6}\) model evaluations even for a simple model of forest carbon balance \citep{zobitz2011primer, ziehn2012}. These methods become infeasible for global implementations of more complex models. 

Whereas variational and MCMC techniques assimilate all available observations over some time window at once, sequential algorithms update the model trajectory at each observation time. These algorithms approximate the BLUE formula in equation~\eqref{chap3:eqn:blue} to update the model parameter and state values whenever an observation is available. This means that parameter values can change over time and state and parameter analysis trajectories will become discontinuous (unless using a sequential `smoother' method). The first sequential method for linear systems was the Kalman Filter (KF) \citep{kalman1960}. The KF method requires the evolution of the error covariance matrix \textbf{B} through the time window as observations are assimilated. This becomes infeasible for large systems. The Ensemble Kalman Filter (EnKF) \citep{Evensen2003} was developed to address this problem and allow for the optimisation of nonlinear systems. Here the error covariance matrix for the state/parameters is approximated using an ensemble of state/parameter vectors. Therefore the evolution of the error covariance matrix \textbf{B} is avoided. These methods are also easy to implement. However, dependent on the complexity of the model, the ensemble size can be limited by computational cost, meaning that covariances can be subject to sampling errors. Ad hoc techniques (localisation and inflation) have been employed to reduce these problems \citep{Hamill2001, andersonandanderson1999}.

\section{Applications to the carbon cycle}

For numerical weather prediction DA is used predominantly for state estimation. However this is not true for land surface carbon balance models where parameters are much less well understood. Indeed these parameters can change over time within a developing ecosystem or when an ecosystem is subject to a disturbance event. Therefore, the vast majority of current studies use DA to estimate both parameter and state variables.

The use of DA for the estimation of parameter and state variables of ecosystem carbon models has either been at site-level, with flux tower observations and other ancillary data relevant to ecosystem carbon balance, or for global implementations, where often the implied effect of the land surface on atmospheric CO\(_{2}\) observations has been considered. It is important that we improve DA techniques both at site-level and for global implementations.   

\subsection{Site-level applications}

%Chronological order?

%Many MCMC routines, at the global scale these will become increasing difficult to implement due to computational expense
\subsubsection{Early efforts}

Two of the first examples of combining site-level eddy covariance data with models of ecosystem carbon balance were using the Data Assimilation Linked Ecosystem Carbon (DALEC) and SImplified PhotosyNthesis and EvapoTranspiration (SIPNET) models by \citet{williams2005improved} and \citet{braswell2005estimating} respectively. These are both simple process based models of ecosystem carbon dynamics. In \citet{braswell2005estimating} MCMC techniques (based on the Metropolis algorithm) are used to combine half-daily observations of NEE with the SIPNET model. The DA technique is used to estimate initial model parameter and state values as well as the standard deviation in NEE flux observation (found to be approximately \(1~\text{g C m}^{-2}\)). It is shown that NEE has limited ability to constrain some model parameters as the model prediction of NEE is insensitive to these parameters at the time-scales shown in the study (10 years). \citet{williams2005improved} assimilated a more diverse set of daily carbon flux and stock observations from the Metolius ponderosa pine site (Oregon, USA) with the DALEC model. In this study, an EnKF is nested within a quasi-Newton optimisation scheme to find the initial set of parameter and state values that require least correction by the EnKF. The use of variational or MCMC techniques is more common to estimate the initial state and parameter values of a model. \citet{williams2005improved} found large reductions in model prediction error after assimilation. They noted that rare measurements of carbon stocks had limited impact on assimilation results but suggested that longer time-series of these stock measurements will be important to constrain carbon pool turnover rates. They also assimilated modelled GPP from the more complex soil-plant-atmosphere (SPA) model \citep{williams1997predicting} and claimed that this was analogous to satellite derived GPP, as this more complex model was already calibrated for the Metolius forest. They suggested that, based on their results assimilating SPA modelled GPP, in future studies using satellite GPP products would be beneficial.  

\subsubsection{Data assimilation comparison projects} 

As data assimilation became more widespread with models and observations of ecosystem carbon dynamics \citet{trudinger2007optic} conducted the Optimisation InterComparison (OptIC) project to better understand the benefits and issues of different DA implementations. In this study participant researchers used a variety of distinct DA implementations to estimate the parameters of a highly simplified model of terrestrial carbon balance. No single DA method was found to perform better than others and the representation of the cost function was shown to be more important than the method. In different optimisation experiments the representation of error added to pseudo observations was varied (Gaussian, lognormal, temporally correlated distributions, etc.). It was stated that the main criterion for success was accurate specification of errors. In particular, none of the participant researchers made an effort to account for temporally correlated error, which resulted in biased results. \citet{williams2009improving} comment that temporal error correlations between flux measurements on the scale of a day and less are likely to be severe. They suggested that these could be included in the observation error covariance matrix, although they comment that this would be a difficult task. In section~\ref{chap6:sec:corR} we show how these correlations can be included.  

The REgional Flux Estimation eXperiment (REFLEX) was a similar study conducted by \citet{fox2009reflex} using the DALEC model. In this study, 9 participants were asked to combine both synthetic and observed NEE and LAI data with the DALEC model. Again a variety of DA methods were used (although no variational methods). No DA technique performed consistently better than others. Across all methods, the parameters linked directly to GPP and TER were best constrained, while those linked to slower processes (allocation and turnover of fine root and wood carbon pools) were poorly constrained. \citet{fox2009reflex} suggest that observations of slow large carbon pools would add useful constraint to DA schemes and compliment eddy covariance data. It is also discussed that future studies should investigate the importance of prior error estimates (we explore this in Chapter~\ref{chap:error_corrs}). The representation of prior and observational errors are still very basic in the majority of current DA schemes for ecosystem carbon balance. \citet{Dietze2013} also stress the need to improve the representation of uncertainty in DA schemes. 

As data assimilation with ecological applications becomes more prevalent it is important that tools for information management and data assimilation are made more accessible. The Predictive Ecosystem Analyser (PEcAn) is an effort to achieve this. PEcAn also allows for easier comparison of different implemented models \citep{Dietze2013} with the aim of improving the standard and reproducibility of experimental results.

\subsubsection{Use of Earth observation data}

Satellite observations of reflectance have also been used with these simple models to assess their impact on modelled estimates. \citet{Quaife2008} used earth observation data from the MODIS instrument on NASA's TERRA and AQUA satellites in an EnKF with the DALEC model at the Metolius forest (Oregon, USA). They found that, after assimilation of MODIS data, modelled LAI was over-predicted when compared to site-level estimates. Over-prediction of LAI led to an over-estimate in both GPP and TER. Despite this, the modelled NEE was improved after assimilation when compared to site flux tower observations and significant reductions in modelled flux uncertainties were achieved. 

Satellite data has also been used with the SIPNET model. \citet{zobitz2014joint} assimilated earth observation data with flux tower NEE on different timescales. Through a combination of assimilation studies and use of the Bayesian information criterion \citep{schwarz1978estimating} to measure information content, they show that the best combination of observations is remotely sensed annually averaged fraction of absorbed photosynthetically active radiation with twice-daily observations of NEE. 

\subsubsection{Current challenges}    

The ecosystem carbon models of SIPNET and DALEC have both been used in many other experiments combining a variety of observations relevant to the carbon balance of terrestrial ecosystems \citep{Zobitz2008, Moore20081467, Sacks2007, Keenan2011}. One problem facing studies working with NEE flux observations alongside other ancillary site-level data is the overweighting of NEE flux data in the assimilation. In general, other site-level measurements are made at longer time-scales. So, the number of NEE flux observations in any given assimilation can outnumber other available observations by a factor of 10 to 1000 (dependent on the time-step of the model). In order to reduce the problem of overweighting flux observations, \citet{richardson2010estimating} used a cost function taking the product of the observation-model missmatches, rather than the sum, to give an absolute, rather than relative, measure of the model fit to observations. This study used MCMC techniques to combine a diverse set of observations from the Howland forest flux site in Maine, USA with the DALEC model. They found in particular that woody biomass accumulation increment provided an orthogonal constraint to NEE data and reduced uncertainties in parameter estimates. In \citet{Keenan2012}, the problem of overweighting NEE in assimilation results was addressed by calculating the model-observation mismatch and then dividing it by the number of data points for each distinct data stream. This problem could also be addressed by better specifying the observation error covariance matrix in the DA scheme. \citet{Keenan2012} used MCMC techniques and the Forest Biomass, Assimilation, Allocation and Respiration (F\"{o}BAAR) model to study the impact of complementary datasets in addition to NEE. \citet{Keenan2013} further investigated the information content in observations using a set of data denial experiments at the Harvard Forest in Massachusetts, USA. They found that data relating to the turnover of carbon pools provides the most information when combined with observations of NEE. \citet{Keenan2013} used true observations to measure information content. It is important to develop new twin experiments and other novel methods to better understand the impact that new unassimilated observations could have on carbon cycle DA results. This will also allow for a more considered approach when planning measurement campaigns. It has also been suggested that effort should be made to define improved observation operators and the specification of their errors \citep{rayner2010current, williams2009improving}, this forms part of the work in Chapter~\ref{chap:disturbance}.

As ecosystem carbon cycle DA is predominantly a parameter estimation problem, equifinality is an ever-present issue, with available data often not being able to constrain all of the optimised model parameters. \citet{Wu01012009} found that only 6 out of 16 model parameters were identifiable, using a conventional MCMC technique to assimilate observations of NEE with a flux-based ecosystem model. In \citet{Bloom2015} a set of ecological ``common sense" dynamical constraints are implemented in a MCMC DA scheme. These are constraints on things such as carbon pool turnover rates and parameter inequalities. These additional constraints act to ensure the retrieved parameter and state values from DA are physically reasonable. Another option for reducing the problem of equifinality would be to better specify the background and observation error covariance matrices so that there is more constraint on data assimilation results. This would be particularly true for the background error covariance matrix where off-diagonal elements would act to enforce balances between different parameter/state values. This is demonstrated in Chapter~\ref{chap:error_corrs}. \citet{ziehn2011} show that the problem of equifinality can be reduced by only optimising parameters for which the available observations provide information, they show that this improves convergence times for data assimilation schemes. It is also important that we continue to produce new distinct sets of observations in order to reduce equifinality further and better understand where model structure can be improved \citep{Carvalhais2010}.


\subsection{Global implementations}

At a similar time to site-level DA implementations with flux tower records, observations of atmospheric CO\(_{2}\) concentration were being used with atmospheric transport models and variational DA methods to perform global inversions and estimate parameters relating to land surface carbon dynamics. An example of this is in \citet{rayner2005two} where 4D-Var is implemented with the Biosphere Energy Transport HYdrology (BETHY) model \citep{knorr2001uncertainties} in a Carbon Cycle Data Assimilation System (CCDAS) to assimilate both satellite observations and atmospheric CO\(_{2}\) concentrations in a stepwise manner on a global scale. It has been shown that, if possible, it is beneficial to assimilate all data streams concurrently rather than in series \citep{macbean2016consistent}, but this may not be practical in some scenarios. In CCDAS, automatic differentiation is used to find the Jacobian and Hessian of the cost function. The inverse Hessian of the cost function is then used to find an estimate to posterior parameter errors \citep{rayner2005two}. They found that uncertainty in long-term soil carbon storage is the largest contributor to uncertainty in net CO\(_{2}\) flux. \citet{scholze2007propagating} show how this estimate to posterior parameter uncertainties from the cost function Hessian can be propagated through time for future modelled predictions. A review of the CCDAS implementation with BETHY can be found in \citet{Kaminski2013}. 

The ORganising Carbon and Hydrology In Dynamic Ecosystems Environment (ORCHIDEE) model \citep{Krinner2005} is a dynamic global vegetation model that has been used in many data assimilation experiments. ORCHIDEE has been used with both sequential \citep{Demarty2007} and variational methods \citep{Bacour2015}. The 4D-Var data assimilation routine for ORCHIDEE outlined in \citet{Kuppel2012} also uses automatic differentiation to find the adjoint of the ORCHIDEE model used in the calculation of the derivative of the cost function. An adjoint has also recently been developed for the Joint UK Land Environment Simulator (JULES) model to allow for the implementation of variational data assimilation \citep{raoult2016land}. Variational techniques have been preferred in these large scale applications due to computational efficiency, with automatic differentiation techniques reducing the time it takes to implement the adjoint of a model. Current variational methods have made the approximation of diagonal background and observation error covariance matrices. %Maybe include Beer 2010?  

Although variational methods have been prevalent in these global implementations, due to computational efficiency, \citet{bloom2016decadal} implemented an MCMC technique (with prior constraints from \citet{Bloom2015}) to find a global \(1^\text{o} \times 1^\text{o}\) DALEC2 map. Using MCMC techniques in this global implementation is possible because DALEC2 is a simple model which requires little computational effort to run. In this study, MODIS LAI and soil carbon observations from the harmonised world soil database were assimilated. Using the ecological dynamical constraints from \citet{Bloom2015} in this global implementation could be an issue. Not all ecosystems will adhere to these constraints (especially if subjected to severe disturbances such as fire or insect outbreak). \citet{bloom2016decadal} used the retrieved global DALEC2 map to gain insight into ecosystem functioning. They suggested that conventional land cover maps cannot adequately describe the spatial variability of carbon states and processes. The results from this study could be used as a set of prior model estimates for variational methods, which may prove more feasible in the long term. 

\section{Summary}

Many efforts and much progress is being made in the field of carbon cycle DA. Currently there are areas that need addressing; the specification of errors, the information content in available and possible new data streams and continued application of DA to new problems involving the carbon cycle are all important areas for progress. Here we discuss three major challenges:

\begin{itemize}

\item Equifinality: Many different combinations of parameters and state values are able to recreate assimilated observations. As discussed, data assimilation for the carbon cycle is both a parameter and state estimation problem. Available data does not allow for all parameters to be identifiable \citep{Luo2009}. The majority of observations in many experiments are NEE flux measurements. These measurements represent the difference between two large fluxes (GPP and TER). Therefore both GPP and TER can be grossly misspecified by a model but still achieve the observed NEE, contributing to the problem of equifinality. It is important that new methods and observations are produced to reduce this issue.

\item Understanding the Information content in current and potential observations: In order to reduce the problem of equifinality, it is important to combine as many distinct data streams as possible. It is of great importance that we understand the information content in potential new data streams, so that we can focus efforts on campaigns that will add the most information possible to DA schemes. In particular we need to understand what measurements best compliment eddy covariance data \citep{rayner2010current, williams2009improving}.

\item Representation of prior and observational errors: Current DA schemes take a very simple approach to defining errors. Many of the studies reviewed here comment on the need to better characterise uncertainties. Improving the representation of prior errors in DA schemes will also help reduce the problem of equifinality by adding extra constraint and imposing balances on assimilation results. It is important that more efforts are made to fully characterise all sources of uncertainty \citep{Keenan2011, raupach2005model}. \citet{Dietze2013} comment that tools for information management and DA need to be more accessible and reproducible. This could also aid the improved characterisation of uncertainties.
\end{itemize}

In this thesis, we choose to work with the 4D-Var data assimilation method. This allows us to use measures of information content that require the derivative of the model code. It allows us to specify different covariance structures in both the background and observation error covariance matrices. Although this PhD is more concerned with site-level implementations, it is also applicable to larger scale DA implementations for the carbon cycle. 

%\begin{itemize}
%\item Importance of forest ecosystems to the carbon cycle and negating human induced climate change. The increasing number of available observations relevant to understanding the carbon balance of forests.

%\item Many efforts made to combine observations with models to improve our understanding of forest ecosystems, currently not clear which observations provide the most information. Different types of data assimilation.

%\item In NWP many efforts made to understand the information content in different sets of observations. Use data assimilation scheme to assess the impact of different observations.

%\item Currently in forest carbon model data assimilation schemes correlations between observation errors and background estimate errors have been ignored. It has been shown in NWP that this can lead to lose of information and unrealistic estimates.

%\item Some more stuff!
%\end{itemize} 
