\documentclass[12pt]{article}
\usepackage[sort]{natbib}
\usepackage{bm,amsmath,bbm,amsfonts,nicefrac,latexsym,amsmath,amsfonts,amsbsy,amscd,amsxtra,amsgen,amsopn,bbm,amsthm,amssymb,graphicx}
\usepackage{fancyhdr}
\usepackage[margin=1.0in]{geometry}

\title{Data assimilation with carbon cycle models}
\author{Ewan Pinnington}

\newtheorem{theorem}{Theorem}[section]
\newtheorem*{defn}{Definition}


\begin{document}

\maketitle

\section{Introduction}

This chapter reviews recent efforts in using data assimilation with carbon cycle models in order to improve current estimates of ecosystem carbon balance.

\section{Data assimilation methods}

Data assimilation provides techniques for combining observations and prior knowledge of a system in an optimal way to find a consistent solution referred to as the analysis. The prior knowledge of a system often takes the form of a numerical model and an initial guess of the model state/parameters. Many statistical methods have been developed for data assimilation. These methods can largely be categorised as either sequential or variational. Sequential algorithms solve the system of equations needed to find an optimal solution explicitly at each observation time. Variational methods solve the equations needed for an optimal solution implicitly by minimising a cost function for all available observations over some time window. This thesis is mainly concerned with the variational technique of four-dimensional variational data assimilation (4D-Var). In numerical weather prediction data assimilation has been predominately used for state estimation whilst keeping parameters fixed. This is because numerical weather prediction is mainly dependent on the initial state with model physics being well understood. Ecosystem carbon cycle models are more dependent on finding the correct set of parameters to describe the ecosystem of interest \citep{luo2015predictability}. We therefore discuss data assimilation for joint state and parameter estimation. In the next sections we give a general introduction to data assimilation, then expand this to 4D-Var and finally we briefly discuss other data assimilation methods not directly used in this thesis but applicable to subsequent discussion. 

\subsection{Introduction to data assimilation}

We consider a system that can be described by a numerical model with a true model state \(\textbf{z}^{t} \in \mathbb{R}^{n}\) and true parameters \(\textbf{p}^{t} \in \mathbb{R}^{q}\). We then define the true augmented state as
\begin{equation}
\textbf{x}^{t} =
\begin{pmatrix}
\textbf{p}^{t} \\
\textbf{z}^{t}
\end{pmatrix}
\in \mathbb{R}^{q+n}.
\end{equation}
The initial guess to this model augmented state \(\textbf{x}^{b} \in \mathbb{R}^{q+n}\) (often referred to as the prior or background) and observations of the system \(\textbf{y} \in \mathbb{R}^{m}\) will only be approximations to the true system state, such that
\begin{equation}
\textbf{x}^{b} = \textbf{x}^{t} + \bm{\epsilon}^{b}, \label{eqn:xb}
\end{equation} 
\begin{equation}
\textbf{y} = h(\textbf{x}^{t}) + \bm{\epsilon}^{o}, \label{eqn:y}
\end{equation} 
where \( \bm{\epsilon}^{b}\) and \( \bm{\epsilon}^{o}\) are the prior and observation errors respectively, and \(h: \mathbb{R}^{q+n}\rightarrow \mathbb{R}^{m}\) is the observation operator (can be linear or non-linear) mapping the augmented state to the observations. The errors in the prior and observations are assumed to be unbiased and mutually independent with known covariance matrices \(\textbf{B} = \mathbb{E}[\bm{\epsilon}^{b}(\bm{\epsilon}^{b})^{T}]\) and \(\textbf{R} = \mathbb{E}[\bm{\epsilon}^{o}(\bm{\epsilon}^{o})^{T}]\).

The best estimate to \(\textbf{x}^{t}\) satisfying both equation~\eqref{eqn:xb} and \eqref{eqn:y} is often called the analysis or the maximum a posteriori estimate, here denoted \(\textbf{x}^{a}\). It is possible to derive this analysis by applying Bayesian methods to probability density functions. Bayes' theorem, first discussed in \citet{bayes1763} but formalised by \citet{laplace1781memoire}, states that the posterior probability of event A given that event B occurs, is proportional to the prior probability of A multiplied by the probability of event B given that event A occurs, this can be expressed mathematically as
\begin{equation}
\mathbb{P}(A|B) \propto \mathbb{P}(A)\mathbb{P}(B|A). \label{eqn:bayes}
\end{equation}
For data assimilation event A represents the augmented state of the system \(\textbf{x}\) and event B the observations \(\textbf{y}\). Maximising the probability \(\mathbb{P}(A|B)\) is then equivalent to finding the augmented state that best represents the observations. 

If we make the assumption of Gaussian probability density functions (pdf) with
\begin{equation}
\mathbb{P}^{b}(\textbf{x}) = \frac{1}{\sqrt{|2\pi\textbf{B}|}}\text{exp}\big(-\frac{1}{2}(\textbf{x}-\textbf{x}^{b})^{T}\textbf{B}^{-1}(\textbf{x}-\textbf{x}^{b})\big)
\end{equation}
and
\begin{equation}
\mathbb{P}^{o}(\textbf{y}|\textbf{x}) = \frac{1}{\sqrt{|2\pi\textbf{R}|}}\text{exp}\big(-\frac{1}{2}(\textbf{y}-h(\textbf{x}))^{T}\textbf{R}^{-1}(\textbf{y}-h(\textbf{x}))\big),
\end{equation}
where \(\mathbb{P}^{b}(\textbf{x})\) is the pdf for the prior and \(\mathbb{P}^{o}(\textbf{y}|\textbf{x})\) the pdf of the observations given the augmented state. Then from Bayes' theorem (equation~\eqref{eqn:bayes}) the posterior pdf for the augmented state
\begin{equation}
\begin{split}
\mathbb{P}^{a}(\textbf{x}|\textbf{y}) &\propto \frac{1}{\sqrt{|2\pi\textbf{B}|}\sqrt{|2\pi\textbf{R}|}}\text{exp}\big(-\frac{1}{2}(\textbf{x}-\textbf{x}^{b})^{T}\textbf{B}^{-1}(\textbf{x}-\textbf{x}^{b})-\frac{1}{2}(\textbf{y}-h(\textbf{x}))^{T}\textbf{R}^{-1}(\textbf{y}-h(\textbf{x}))\big) \\
&\propto \text{exp}\big(-\frac{1}{2}(\textbf{x}-\textbf{x}^{b})^{T}\textbf{B}^{-1}(\textbf{x}-\textbf{x}^{b})-\frac{1}{2}(\textbf{y}-h(\textbf{x}))^{T}\textbf{R}^{-1}(\textbf{y}-h(\textbf{x}))\big), \label{eqn:p_x_y}
\end{split}
\end{equation} 
here we can ignore the constant multiplying the exponential function as it is independent of \textbf{x}. We want to maximise the probability of the augmented state \textbf{x} given the observations \textbf{y}, from equation~\eqref{eqn:p_x_y} we can see that to maximise \(\mathbb{P}^{a}(\textbf{x}|\textbf{y})\) we must maximise the terms in the exponent, this is equivalent to minimising the quadratic cost function 
\begin{equation}
J(\textbf{x}) = \frac{1}{2}(\textbf{x}-\textbf{x}^{b})^{T}\textbf{B}^{-1}(\textbf{x}-\textbf{x}^{b}) + \frac{1}{2}(\textbf{y}-h(\textbf{x}))^{T}\textbf{R}^{-1}(\textbf{y}-h(\textbf{x})). \label{eqn:3dvar}
\end{equation}
This is the cost function minimised in three-dimensional variational data assimilation (3D-Var), where the minimum is found using a descent algorithm evaluating equation~\eqref{eqn:3dvar} and its gradient \citep{courtier1998ecmwf}. We can approximate the minimum of \eqref{eqn:3dvar} by finding its gradient and setting it to zero to obtain the best linear unbiased estimate (BLUE) \citep{talagrand1997assimilation} where
\begin{equation}
\textbf{x}^{a} = \textbf{x}^{b} + \textbf{K}(\textbf{y} - h(\textbf{x}^{b})), \label{eqn:blue}
\end{equation}
\begin{equation}
\textbf{K} = \textbf{B}\textbf{H}^{T}(\textbf{H}\textbf{B}\textbf{H}^{T}+\textbf{R})^{-1},
\end{equation}
where \textbf{K} is the Kalman gain matrix specifying the weight of the analysis increment and \(\textbf{H}=\frac{\partial h(\textbf{x})}{\partial \textbf{x}}\) is the linearised observation operator. We can also approximate the analysis error covariance matrix as
\begin{equation}
\textbf{A} = (\textbf{H}^{T}\textbf{R}^{-1}\textbf{H}+\textbf{B}^{-1})^{-1}, \label{eqn:a_cov}
\end{equation}
if \( h \) is linear then \eqref{eqn:blue} and \eqref{eqn:a_cov} are exact solutions.
%(BLUE) introduce basic concept of DA for a linear Gaussian time-invariant system... will use this in Info Con chapter

%BLUE \(\rightarrow\) 3D-Var

\subsection{4D-Var}

%*Brief* but inclusive of all notation need for results chapter on information content
4D-Var extends 3D-Var to allow for the assimilation of observations distributed throughout some time interval \(t_{0}\) to \(t_{N}\). \citet{Sasaki70somebasic} proposed a method for combining a time series of observations with a numerical model, this was then further developed for use in numerical weather prediction \citep{dimet1986variational}. In 4D-Var we minimise the cost function,

\begin{equation}
J(\textbf{x}_0) = \frac{1}{2}(\textbf{x}_0-\textbf{x}^b)^{T}\textbf{B}^{-1}(\textbf{x}_0-\textbf{x}^b)+\frac{1}{2}\sum_{i=0}^{N}(\textbf{y}_i-\textbf{h}_i(\textbf{x}_i))^{T}\textbf{R}_{i}^{-1}(\textbf{y}_i-\textbf{h}_i(\textbf{x}_i)), \label{eqn:4dvar_cost}
\end{equation}

to obtain the analysis \(\textbf{x}^{a}_{0}\), valid at the initial time \(t_{0}\), subject to the strong constraint that the model states (\(\textbf{x}_0, \dots, \textbf{x}_N\)) must satisfy the model equations,

\begin{equation}
\textbf{x}_{i} = \textbf{m}_{i-1 \rightarrow i}(\textbf{x}_{i-1}), \label{eqn:nonlinmod}
\end{equation}

where \(\textbf{x}_{i}\) is the model augmented state at time \(t_i\), \(\textbf{m}_{i-1 \rightarrow i}\) is the possibly nonlinear augmented system model evolving \(\textbf{x}_{i-1}\) from time \(t_{i-1}\) to time \(t_i\), \(\textbf{y}_i\) is the vector of observations at time \(t_i\), and \(h_i\) is the observation operator at time \(t_i\). We can rewrite equation~\eqref{eqn:4dvar:cost} to avoid the sum notation as

\begin{equation}
J(\textbf{x}_0) = \frac{1}{2}(\textbf{x}_0-\textbf{x}^b)^{T}\textbf{B}^{-1}(\textbf{x}_0-\textbf{x}^b)+\frac{1}{2}(\hat{\textbf{y}}-\hat{\textbf{h}}(\textbf{x}_0))^{T}\hat{\textbf{R}}^{-1}(\hat{\textbf{y}}-\hat{\textbf{h}}(\textbf{x}_0)) \label{costfn}
\end{equation}


where,

\begin{equation}
\hat{\textbf{y}} =
\begin{pmatrix}
\textbf{y}_0 \\
\textbf{y}_1\\
\vdots \\
\textbf{y}_N
\end{pmatrix},
\hspace{1mm}
\hat{\textbf{h}}(\textbf{x}_0)=
\begin{pmatrix}
\textbf{h}_0(\textbf{x}_0) \\
\textbf{h}_1(\textbf{m}_{0\rightarrow 1}(\mathbf{x}_{0}))\\
\vdots \\
\textbf{h}_N(\textbf{m}_{0\rightarrow N}(\mathbf{x}_{0}))
\end{pmatrix}, \\
\hspace{1mm} \text{and} \hspace{3mm}
\hat{\mathbf{R}} =
\begin{pmatrix}
\mathbf{R}_{0, 0} & \mathbf{R}_{0, 1} & \dots & \mathbf{R}_{0, N} \\
\mathbf{R}_{1, 0} & \mathbf{R}_{1, 1} & \dots & \mathbf{R}_{1, N} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{R}_{N, 0} & \mathbf{R}_{N, 1} & \dots & \mathbf{R}_{N, N}
\end{pmatrix}.
\end{equation}
For 4D-Var we approximate the analysis error covariance matrix as
\begin{equation}
\textbf{A} = (\hat{\textbf{H}}^{T}\hat{\textbf{R}}^{-1}\hat{\textbf{H}}+\textbf{B}^{-1})^{-1}, \label{eqn:a_cov_4dvar}
\end{equation}
where \(\hat{\textbf{H}}\) is that observability matrix given by
\begin{equation}
\hat{\mathbf{H}}=
\begin{pmatrix}
\mathbf{H}_0 \\
\mathbf{H}_1\mathbf{M}_0\\
\vdots \\
\mathbf{H}_N\mathbf{M}_{N,0}
\end{pmatrix}.
\end{equation}
\subsection{Sequential and Monte-Carlo approaches}

Brief overview of these methods.

Sequential algorithms approximate the BLUE formula to update the model trajectory at each observation time. 

\section{Applications to the carbon cycle}

DA for NWP is considered a state estimation problem as the physics of the problem are well understood and therefore parameterisations should not change over time. For the C cycle DA is more of a joint parameter and state estimation problem with the vast majority of studies using DA to estimate both parameter and state variables for a given system. Parameters governing land surface C uptake can change over time with developing forest and disturbance events.  

\subsection{Site-level applications}

Many MCMC routines, at the global scale these will become increasing difficult to implement due to computational expense

\subsection{Global inversions}

adjoints used for faster optimisation... also allows for finding posterior error distributions and propagating these to future estimates [Scholze]

Current efforts use a stepwise approach to assimilate different distinct data streams... this has been shown to not be optimal [MacBean]

\subsection{Issues faced in C cycle DA}

\begin{itemize}
\item Equifinality: Many different combinations of parameters and states able to recreate assimilated obs.

\item Information content in obs: In order to reduce the problem of equifinality it is important to combine as many distinct data streams as possible, it is of great importance that we understand the information content in potential new data streams so that we can focus efforts on campaigns that will add the most information possible to DA schemes. Important to understand what measurements best compliment EC data.

\item Representation of prior and observational errors: Current DA schemes take a very simple approach to defining errors. Improving the representation of error in DA schemes will also help reduce the problem of equifinality.  
\end{itemize}

\section{Conclusion} 

Many efforts and much progress being made in the field of C cycle DA. Currently there are areas that need addressing... the specification of errors, information content in available and possible new data streams and continued application of DA to new problems involving the C cycle are all important areas for progress...

%\begin{itemize}
%\item Importance of forest ecosystems to the carbon cycle and negating human induced climate change. The increasing number of available observations relevant to understanding the carbon balance of forests.

%\item Many efforts made to combine observations with models to improve our understanding of forest ecosystems, currently not clear which observations provide the most information. Different types of data assimilation.

%\item In NWP many efforts made to understand the information content in different sets of observations. Use data assimilation scheme to assess the impact of different observations.

%\item Currently in forest carbon model data assimilation schemes correlations between observation errors and background estimate errors have been ignored. It has been shown in NWP that this can lead to lose of information and unrealistic estimates.

%\item Some more stuff!
%\end{itemize} 

\bibliographystyle{abbrvnat}
\bibliography{../PhD}{}

\end{document}