\documentclass[12pt]{article}
\usepackage[sort]{natbib}
\usepackage{bm,amsmath,bbm,amsfonts,nicefrac,latexsym,amsmath,amsfonts,amsbsy,amscd,amsxtra,amsgen,amsopn,bbm,amsthm,amssymb,graphicx, color}
\usepackage{fancyhdr}
\usepackage[margin=1.0in]{geometry}
\usepackage{setspace}
%\doublespacing

\title{Data assimilation with carbon cycle models {\color{red} DRAFT}}
\author{Ewan Pinnington}

\newtheorem{theorem}{Theorem}[section]
\newtheorem*{defn}{Definition}


\begin{document}

\maketitle

%\section{Introduction}

%This chapter reviews recent efforts in using data assimilation with carbon cycle models in order to improve current estimates of ecosystem carbon balance.

\section{Data assimilation methods}

Data assimilation provides techniques for combining observations and prior knowledge of a system in an optimal way to find a consistent solution referred to as the analysis. The prior knowledge of a system often takes the form of a numerical model and an initial guess of the model state/parameters. Many statistical methods have been developed for data assimilation. These methods can largely be categorised as either sequential or variational. Sequential algorithms solve the system of equations needed to find an optimal solution explicitly at each observation time. Variational methods solve the equations needed for an optimal solution implicitly by minimising a cost function for all available observations over some time window. This thesis is mainly concerned with the variational technique of four-dimensional variational data assimilation (4D-Var). In numerical weather prediction data assimilation has been predominately used for state estimation whilst keeping parameters fixed. This is because numerical weather prediction is mainly dependent on the initial state with model physics being well understood. Ecosystem carbon cycle models are more dependent on finding the correct set of parameters to describe the ecosystem of interest \citep{luo2015predictability}. We therefore discuss data assimilation for joint state and parameter estimation. In the next sections we give a general introduction to data assimilation, then expand this to 4D-Var and finally we briefly discuss other data assimilation methods not directly used in this thesis but applicable to subsequent discussion. 

\subsection{Introduction to data assimilation}

We consider a system that can be described by a numerical model with a true model state \(\textbf{z}^{t} \in \mathbb{R}^{n}\) and true parameters \(\textbf{p}^{t} \in \mathbb{R}^{q}\). We then define the true augmented state as
\begin{equation}
\textbf{x}^{t} =
\begin{pmatrix}
\textbf{p}^{t} \\
\textbf{z}^{t}
\end{pmatrix}
\in \mathbb{R}^{q+n}.
\end{equation}
The initial guess to this model augmented state \(\textbf{x}^{b} \in \mathbb{R}^{q+n}\) (often referred to as the prior or background) and observations of the system \(\textbf{y} \in \mathbb{R}^{m}\) will only be approximations to the true system state, such that
\begin{equation}
\textbf{x}^{b} = \textbf{x}^{t} + \bm{\epsilon}^{b}, \label{eqn:xb}
\end{equation} 
\begin{equation}
\textbf{y} = h(\textbf{x}^{t}) + \bm{\epsilon}^{o}, \label{eqn:y}
\end{equation} 
where \( \bm{\epsilon}^{b}\) and \( \bm{\epsilon}^{o}\) are the prior and observation errors respectively, and \(h: \mathbb{R}^{q+n}\rightarrow \mathbb{R}^{m}\) is the observation operator (can be linear or non-linear) mapping the augmented state to the observations. The errors in the prior and observations are assumed to be unbiased and mutually independent with known covariance matrices \(\textbf{B} = \mathbb{E}[\bm{\epsilon}^{b}(\bm{\epsilon}^{b})^{T}]\) and \(\textbf{R} = \mathbb{E}[\bm{\epsilon}^{o}(\bm{\epsilon}^{o})^{T}]\).

The best estimate to \(\textbf{x}^{t}\) satisfying both equation~\eqref{eqn:xb} and \eqref{eqn:y} is often called the analysis or the maximum a posteriori estimate, here denoted \(\textbf{x}^{a}\). It is possible to derive this analysis by applying Bayesian methods to probability density functions. Bayes' theorem, first discussed in \citet{bayes1763} but formalised by \citet{laplace1781memoire}, states that the posterior probability of event A given that event B occurs, is proportional to the prior probability of A multiplied by the probability of event B given that event A occurs, this can be expressed mathematically as
\begin{equation}
\mathbb{P}(A|B) \propto \mathbb{P}(A)\mathbb{P}(B|A). \label{eqn:bayes}
\end{equation}
For data assimilation event A represents the augmented state of the system \(\textbf{x}\) and event B the observations \(\textbf{y}\). Maximising the probability \(\mathbb{P}(A|B)\) is then equivalent to finding the augmented state that best represents the observations. 

If we make the assumption of Gaussian probability density functions (pdf) with
\begin{equation}
\mathbb{P}^{b}(\textbf{x}) = \frac{1}{\sqrt{|2\pi\textbf{B}|}}\text{exp}\big(-\frac{1}{2}(\textbf{x}-\textbf{x}^{b})^{T}\textbf{B}^{-1}(\textbf{x}-\textbf{x}^{b})\big)
\end{equation}
and
\begin{equation}
\mathbb{P}^{o}(\textbf{y}|\textbf{x}) = \frac{1}{\sqrt{|2\pi\textbf{R}|}}\text{exp}\big(-\frac{1}{2}(\textbf{y}-h(\textbf{x}))^{T}\textbf{R}^{-1}(\textbf{y}-h(\textbf{x}))\big),
\end{equation}
where \(\mathbb{P}^{b}(\textbf{x})\) is the pdf for the prior and \(\mathbb{P}^{o}(\textbf{y}|\textbf{x})\) the pdf of the observations given the augmented state. Then from Bayes' theorem (equation~\eqref{eqn:bayes}) the posterior pdf for the augmented state
\begin{equation}
\begin{split}
\mathbb{P}^{a}(\textbf{x}|\textbf{y}) &\propto \frac{1}{\sqrt{|2\pi\textbf{B}|}\sqrt{|2\pi\textbf{R}|}}\text{exp}\big(-\frac{1}{2}(\textbf{x}-\textbf{x}^{b})^{T}\textbf{B}^{-1}(\textbf{x}-\textbf{x}^{b})-\frac{1}{2}(\textbf{y}-h(\textbf{x}))^{T}\textbf{R}^{-1}(\textbf{y}-h(\textbf{x}))\big) \\
&\propto \text{exp}\big(-\frac{1}{2}(\textbf{x}-\textbf{x}^{b})^{T}\textbf{B}^{-1}(\textbf{x}-\textbf{x}^{b})-\frac{1}{2}(\textbf{y}-h(\textbf{x}))^{T}\textbf{R}^{-1}(\textbf{y}-h(\textbf{x}))\big), \label{eqn:p_x_y}
\end{split}
\end{equation} 
here we can ignore the constant multiplying the exponential function as it is independent of \textbf{x}. We want to maximise the probability of the augmented state \textbf{x} given the observations \textbf{y}, from equation~\eqref{eqn:p_x_y} we can see that to maximise \(\mathbb{P}^{a}(\textbf{x}|\textbf{y})\) we must maximise the terms in the exponent, this is equivalent to minimising the quadratic cost function 
\begin{equation}
J(\textbf{x}) = \frac{1}{2}(\textbf{x}-\textbf{x}^{b})^{T}\textbf{B}^{-1}(\textbf{x}-\textbf{x}^{b}) + \frac{1}{2}(\textbf{y}-h(\textbf{x}))^{T}\textbf{R}^{-1}(\textbf{y}-h(\textbf{x})). \label{eqn:3dvar}
\end{equation}
This is the cost function minimised in three-dimensional variational data assimilation (3D-Var), where the minimum is found using a descent algorithm evaluating equation~\eqref{eqn:3dvar} and its gradient \citep{courtier1998ecmwf}. We can approximate the minimum of \eqref{eqn:3dvar} by finding its gradient and setting it to zero to obtain the best linear unbiased estimate (BLUE) \citep{talagrand1997assimilation} where
\begin{equation}
\textbf{x}^{a} = \textbf{x}^{b} + \textbf{K}(\textbf{y} - h(\textbf{x}^{b})), \label{eqn:blue}
\end{equation}
\begin{equation}
\textbf{K} = \textbf{B}\textbf{H}^{T}(\textbf{H}\textbf{B}\textbf{H}^{T}+\textbf{R})^{-1},
\end{equation}
where \textbf{K} is the Kalman gain matrix specifying the weight of the analysis increment and \(\textbf{H}=\frac{\partial h(\textbf{x})}{\partial \textbf{x}}\) is the linearised observation operator. We can also approximate the analysis error covariance matrix as
\begin{equation}
\textbf{A} = (\textbf{H}^{T}\textbf{R}^{-1}\textbf{H}+\textbf{B}^{-1})^{-1}, \label{eqn:a_cov}
\end{equation}
if \( h \) is linear then \eqref{eqn:blue} and \eqref{eqn:a_cov} are exact solutions.
%(BLUE) introduce basic concept of DA for a linear Gaussian time-invariant system... will use this in Info Con chapter

%BLUE \(\rightarrow\) 3D-Var

\subsection{4D-Var}

%*Brief* but inclusive of all notation need for results chapter on information content
4D-Var extends 3D-Var to allow for the assimilation of observations distributed throughout some time interval \(t_{0}\) to \(t_{N}\). \citet{Sasaki70somebasic} proposed a method for combining a time series of observations with a numerical model, which was then further developed for use in numerical weather prediction \citep{dimet1986variational}. In 4D-Var we minimise the cost function,

\begin{equation}
J(\textbf{x}_0) = \frac{1}{2}(\textbf{x}_0-\textbf{x}^b)^{T}\textbf{B}^{-1}(\textbf{x}_0-\textbf{x}^b)+\frac{1}{2}\sum_{i=0}^{N}(\textbf{y}_i-\textbf{h}_i(\textbf{x}_i))^{T}\textbf{R}_{i}^{-1}(\textbf{y}_i-\textbf{h}_i(\textbf{x}_i)), \label{eqn:4dvar_cost}
\end{equation}

to obtain the analysis \(\textbf{x}^{a}_{0}\), valid at the initial time \(t_{0}\), subject to the strong constraint that the model states (\(\textbf{x}_0, \dots, \textbf{x}_N\)) must satisfy the model equations,

\begin{equation}
\textbf{x}_{i} = \textbf{m}_{i-1 \rightarrow i}(\textbf{x}_{i-1}), \label{eqn:nonlinmod}
\end{equation}

where \(\textbf{x}_{i}\) is the model augmented state at time \(t_i\), \(\textbf{m}_{i-1 \rightarrow i}\) is the possibly nonlinear augmented system model evolving \(\textbf{x}_{i-1}\) from time \(t_{i-1}\) to time \(t_i\), \(\textbf{y}_i\) is the vector of observations at time \(t_i\), and \(h_i\) is the observation operator at time \(t_i\). We can rewrite equation~\eqref{eqn:4dvar_cost} to avoid the sum notation as

\begin{equation}
J(\textbf{x}_0) = \frac{1}{2}(\textbf{x}_0-\textbf{x}^b)^{T}\textbf{B}^{-1}(\textbf{x}_0-\textbf{x}^b)+\frac{1}{2}(\hat{\textbf{y}}-\hat{\textbf{h}}(\textbf{x}_0))^{T}\hat{\textbf{R}}^{-1}(\hat{\textbf{y}}-\hat{\textbf{h}}(\textbf{x}_0)) \label{costfn}
\end{equation}


where,

\begin{equation}
\hat{\textbf{y}} =
\begin{pmatrix}
\textbf{y}_0 \\
\textbf{y}_1\\
\vdots \\
\textbf{y}_N
\end{pmatrix},
\hspace{1mm}
\hat{\textbf{h}}(\textbf{x}_0)=
\begin{pmatrix}
\textbf{h}_0(\textbf{x}_0) \\
\textbf{h}_1(\textbf{m}_{0\rightarrow 1}(\mathbf{x}_{0}))\\
\vdots \\
\textbf{h}_N(\textbf{m}_{0\rightarrow N}(\mathbf{x}_{0}))
\end{pmatrix}, \\
\hspace{1mm} \text{and} \hspace{3mm}
\hat{\mathbf{R}} =
\begin{pmatrix}
\mathbf{R}_{0, 0} & \mathbf{R}_{0, 1} & \dots & \mathbf{R}_{0, N} \\
\mathbf{R}_{1, 0} & \mathbf{R}_{1, 1} & \dots & \mathbf{R}_{1, N} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{R}_{N, 0} & \mathbf{R}_{N, 1} & \dots & \mathbf{R}_{N, N}
\end{pmatrix}.
\end{equation}
For 4D-Var we approximate the analysis error covariance matrix as
\begin{equation}
\textbf{A} = (\hat{\textbf{H}}^{T}\hat{\textbf{R}}^{-1}\hat{\textbf{H}}+\textbf{B}^{-1})^{-1}, \label{eqn:a_cov_4dvar}
\end{equation}
where \(\hat{\textbf{H}}\) is that observability matrix given by
\begin{equation}
\hat{\mathbf{H}}=
\begin{pmatrix}
\mathbf{H}_0 \\
\mathbf{H}_1\mathbf{M}_0\\
\vdots \\
\mathbf{H}_N\mathbf{M}_{N,0}
\end{pmatrix}
\end{equation}
with $\textbf{H}_i = \frac{\partial \textbf{h}_i(\textbf{x}_i)}{\partial\textbf{x}_i}$ the linearised observation operator and $\mathbf{M}_{i,0}=\mathbf{M}_{i-1}\mathbf{M}_{i-2}\cdots\mathbf{M}_0$ the tangent linear model with $\mathbf{M}_i=\frac{\partial \textbf{m}_{i-1\rightarrow i}(\textbf{x}_{i})}{\partial \textbf{x}_{i}}$. The tangent linear model can be difficult to implement, however using techniques such as automatic differentiation \citep{renaud1997automatic} can reduce the time taken to implement the derivative of a model. 

\subsection{Sequential and Markov chain Monte Carlo approaches}

Markov chain Monte Carlo (MCMC) methods refer to a suite of related algorithms (Metropolis-Hastings, simulated annealing and Gibbs sampling), with the first MCMC method being the Metropolis algorithm \citep{metropolis1953equation}. These methods sample a cost function measuring the model-data miss-match, usually similar to the negation of the second term in the 4D-Var cost function shown in equation~\eqref{costfn}. As these methods use the negation of the cost function in equation~\eqref{costfn} they seek to find a global optimum for this cost function rather than a minimum. This is achieved by iteratively sampling the cost function, with each iteration of the parameter and state values being uniquely determined by the previously sampled parameter and state values. The output of the MCMC methods is a set of accepted parameter and state values from which analysis or posterior error covariances can be calculated. These methods are easy to implement and do not require the derivative of the model code. However, they come with high computational cost as they often require in the order of \(10^{6}\) model evaluations \citep{zobitz2011primer}, meaning these methods become infeasible for global implementations of more complex models. 

Whereas variational and MCMC techniques assimilate all available observations over some time window at once, sequential algorithms update the model trajectory at each observation time. These algorithms approximate the BLUE formula in equation~\eqref{eqn:blue} to update the model parameter and state values whenever an observation is available. This means that parameter values can change over time and state and parameter analysis trajectories will become discontinuous (unless using a sequential `smoother' method). The first sequential method was the Kalman Filter (KF) \citep{kalman1960}. The KF method requires the evolution of the error covariance matrix \textbf{B} through the time window as observations are assimilated. This becomes infeasible for large systems. The Ensemble Kalman Filter (EnKF) \citep{Evensen2003} was therefore developed to address this problem, the error covariance matrix for the state/parameters is approximated using an ensemble of state/parameter vectors therefore the evolution of the error covariance matrix \textbf{B} is avoided. These methods are also easy to implement, however, dependent on the complexity of the model, the ensemble size can be limited by computational cost, meaning that covariances can be subject to noise and techniques have to be employed to avoid this. The ensemble can also collapse on the same value after assimilating a number of observations, this can be avoided by adding random noise to the system [{\color{red}REF look at Sarah's DA notes}]. 


\section{Applications to the carbon cycle}

DA for NWP is considered a state estimation problem as the physics of the problem are well understood and therefore parameterisations should not change over time. For the C cycle DA is more of a joint parameter and state estimation problem with the vast majority of studies using DA to estimate both parameter and state variables for a given system. Parameters governing land surface C uptake can change over time with developing forest and disturbance events.  

\subsection{Site-level applications}

%Chronological order?

%Many MCMC routines, at the global scale these will become increasing difficult to implement due to computational expense

Two of the first examples of combining site-level eddy covariance data with models of ecosystem carbon balance were using the Data Assimilation Linked Ecosystem Carbon (DALEC) and SImplified PhotosyNthesis and EvapoTranspiration (SIPNET) models in \citet{williams2005improved} and \citet{braswell2005estimating} respectively. These are both simple process based models of ecosystem carbon dynamics. In \citet{braswell2005estimating} MCMC techniques (based on the Metropolis algorithm) are used to combine half-daily observations of NEE with the SIPNET model. The DA technique is used to estimate initial model parameter and state values as well as the standard deviation in NEE flux observation (found to be approximately \(1~\text{g C m}^{-2}\)). It is shown that NEE has limited ability in constraining some model parameters as the model prediction of NEE is insensitive to these parameters at the time-scales shown in the study (10 years). \citet{williams2005improved} assimilate a more diverse set of daily carbon flux and stock observations with the DALEC model at the Metolius ponderosa pine site in Oregon, USA. In this study an EnKF is nested within a quasi-Newton optimisation scheme to find the initial set of parameter and state values that require least correction by the EnKF. As the aim of this study was to estimate the initial state and parameter values of the model it would perhaps be more logical to use a variational or MCMC technique. \citet{williams2005improved} found large reductions in model prediction error after assimilation, noting that rare measurements of carbon stocks have limited impact on assimilation results but suggesting that longer time-series of these stock measurements will be important to constrain carbon pool turnover rates. Assimilate modelled GPP {\color{red}REWORD?} suggest analogous to satellite derived GPP and that this could help improve results if used in future studies.   

As data assimilation became more widespread with models and observations of ecosystem carbon dynamics \citet{trudinger2007optic} conducted the Optimisation InterComparison (OptIC) project to better understand the benefits and issues of different DA implementations. In this study participant researchers used a variety of distinct DA implementations to estimate the parameters of a highly simplified model of terrestrial carbon balance. No single DA method was found to perform better than others and the representation of the cost function was shown to be more important than the method. In different optimisation experiments the representation of error added to pseudo observations was varied (Gaussian, lognormal, temporally correlated distributions, etc.). It was stated that the main criterion for success was accurate specification of errors. In particular, none of the participant researchers made an effort to account for temporally correlated error and this resulted in biased results. The REgional Flux Estimation eXperiment (REFLEX) was a similar study conducted using the DALEC model by \citet{fox2009reflex}, here 9 participants were asked to combine both synthetic and observed NEE and LAI data with the DALEC model. Again a variety of DA methods were used (although no variational methods present), with no DA technique performing consistently better than others. Across all methods parameters linked directly to GPP and TER were best constrained, with parameters linked to slower processes (allocation and turnover of fine root and wood carbon pools) being poorly constrained. \citet{fox2009reflex} suggest that observations of slow large carbon pools would add useful constraint to DA schemes and compliment eddy covariance data, it is also discussed that future studies should investigate the importance of prior error estimates. Currently the representation of prior and observational errors are still very basic in the majority of DA schemes for ecosystem carbon balance. [\citep{Dietze2013} also stress the need to improve the representation of uncerainty in DA schemes. As data assimilation with ecological applications becomes more prevalent it is important that tools for information management and data assimilation are made more accessible. The Predictive Ecosystem Analyser (PEcAn) is an effort to achieve this, this system also allows for easier comparison of different implemented models \citep{Dietze2013} improving the standard and reproducibility of experimental results is important. (Make this sit nicely in this paragraph)]

Satellite observations of reflectance have also been used with these simple models to assess their impact on modelled estimates. In \citet{Quaife2008} used earth observation data from the MODIS instrument on NASA's TERRA and AQUA satellites in an EnKF with the DALEC model at the Metolius forest in Oregon, USA. They found that, after assimilation of MODIS data, modelled LAI was over-predicted when compared to site-level estimates. Over-prediction of LAI lead to an over-estimate in both GPP and TER. Despite this modelled NEE is improved after assimilation when compared to site flux tower observations and significant reductions in modelled flux uncertainties are achieved. Satellite data has also been used with the SIPNET model, in \citet{zobitz2014joint} earth observation data is assimilated on different time-scales with flux tower NEE. Through a combination of assimilation studies and use of the Bayesian information criterion \citep{schwarz1978estimating} measure of information content they show that the best combination of observations is remotely sensed annually averaged fraction of absorbed photosynthetically active radiation with twice-daily observations of NEE.     

The ecosystem carbon models of SIPNET and DALEC have both been used in many other experiments combining a variety of observations relevant to the carbon balance of terrestrial ecosystems \citep{Zobitz2008, Moore20081467, Sacks2007, Keenan2011}. One problem facing studies working with NEE flux observations alongside other ancillary site-level data is the overweighting of NEE flux data in the assimilation as in general other site-level measurements are made at longer time-scales so that the number of NEE flux observations in the any given assimilation can outnumber other available observations by anywhere from a factor of 10 to a factor of 1000 (dependent on the time-step of the model). In order to reduce the problem of overweighting flux observations \citet{richardson2010estimating} used a cost function taking the product of the observation-model miss-matches, rather than the sum, to give an absolute rather than relative measure of the model fit to observations. This study used MCMC techniques to combine a diverse set of observations from the Howland forest flux site in Maine, USA with the DALEC model. They found in particular that woody biomass accumulation increment provided an orthogonal constraint to NEE data and reduced uncertainties in parameter estimates. In \citet{Keenan2012} the problem of overweighting NEE in assimilation results is addressed by calculating the model-observation mismatch and then dividing it by the number of data points for each distinct data stream. This problem could also be treated by better specifying the observation error covariance matrix in the DA scheme. \citet{Keenan2012} work with MCMC techniques and the Forest Biomass, Assimilation, Allocation and Respiration (F\"{o}BAAR) model . This study discusses the impact of complimentary datasets in addition to NEE, with \citet{Keenan2013} further investigating the information content in observations using a set of data denial experiments at the Harvard Forest in Massachusetts, USA. They found that data relating to the turnover of carbon pools provides the most information when combined with observation of NEE. This study uses true observations, it is important to develop new twin experiments and other methods to understand the impact of possible observations and allow for more consideration when planning a measurement campaign {\color{red}REWORD}.  

As ecosystem carbon cycle DA is predominantly a parameter estimation problem equifinality is an ever-present issue, with available data often not being able to constrain all of the optimised model parameters. In \citet{Bloom2015} a set of ecological ``common sense" dynamical constraints are implemented in a MCMC DA scheme, these are constraints on things such as carbon pool turnover rates and parameter inequalities. These additional constraints act to ensure the retrieved parameter and state values from DA are physically reasonable. Another option for reducing the problem of equifinality would be to better specify the background and observation error covariance matrices so that there is more constraint on data assimilation results. This would be particularly true for the background error covariance matrix where off-diagonal elements would act to enforce balances between different parameter/state values.    



\subsection{Global inversions}

At a similar time to site-level DA implementations with flux tower records, observations of atmospheric CO\(_{2}\) concentration were being used with atmospheric transport models and variational DA methods to perform global inversions and estimate parameters relating to land surface carbon dynamics. An early example of this is in \citet{rayner2005two} where 4D-Var is implemented with the Biosphere Energy Transport HYdrology (BETHY) model \citep{knorr2001uncertainties} to assimilate both satellite observations and atmospheric CO\(_{2}\) concentrations in a stepwise manner on a global scale. It has been shown that, if possible, it is beneficial to assimilate all data streams concurrently rather than in series \citep{macbean2016consistent}, but this may not be practical in some scenarios. 


Variational techniques used for computational efficeincy, automatic differention allows for calculation of model adjoint...
ORCHIDEE use adjoint? autodiff? or finite differences? 


At a similar time to site implementations with flux tower data, variational techniques began to be used with obs of CO2 concentrations and sat obs of LAI. An early example of this was Rayner et al. 2005 with the BETHY model they used 4dvar stepwise automatic diff...
adjoints used for faster optimisation... also allows for finding posterior error distributions and propagating these to future estimates [Scholze 2007]
Current efforts use a stepwise approach to assimilate different distinct data streams... this has been shown to not be optimal [MacBean 2016]

Also talk about orchidee

More recently site level obs have also been used in these global optimisations JULES Raoult 2016, ORCHIDEE, CCDAS...

Although in these global inversions variational methods have been prevelant due to computational efficency in Bloom et al. 2016 MCMC techniques are used to find global DALEC2 map, this is possible are DALEC is a simple model which requies little computational cost to run, found that... results from this study could be usued as a set of prior model estimates for variational methods which may prove more feasible in the long term.

 

\subsection{Issues faced in C cycle DA}

\begin{itemize}
\item Equifinality: Many different combinations of parameters and states able to recreate assimilated obs. Williams 2009, Luo 2009

\item Information content in obs: In order to reduce the problem of equifinality it is important to combine as many distinct data streams as possible, it is of great importance that we understand the information content in potential new data streams so that we can focus efforts on campaigns that will add the most information possible to DA schemes. Important to understand what measurements best compliment EC data. Rayner 2010

\item Representation of prior and observational errors: Current DA schemes take a very simple approach to defining errors. Improving the representation of error in DA schemes will also help reduce the problem of equifinality. Keenan 2011, Raupach 2004  
[Dietze 2013 comments tools for information management and DA need to be ore accessible, characterisation of uncertainty important]
\end{itemize}

\section{Conclusion} 

Many efforts and much progress being made in the field of C cycle DA. Currently there are areas that need addressing... the specification of errors, information content in available and possible new data streams and continued application of DA to new problems involving the C cycle are all important areas for progress...

%\begin{itemize}
%\item Importance of forest ecosystems to the carbon cycle and negating human induced climate change. The increasing number of available observations relevant to understanding the carbon balance of forests.

%\item Many efforts made to combine observations with models to improve our understanding of forest ecosystems, currently not clear which observations provide the most information. Different types of data assimilation.

%\item In NWP many efforts made to understand the information content in different sets of observations. Use data assimilation scheme to assess the impact of different observations.

%\item Currently in forest carbon model data assimilation schemes correlations between observation errors and background estimate errors have been ignored. It has been shown in NWP that this can lead to lose of information and unrealistic estimates.

%\item Some more stuff!
%\end{itemize} 

\bibliographystyle{abbrvnat}
\bibliography{../PhD}{}

\end{document}